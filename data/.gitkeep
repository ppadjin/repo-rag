from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms import Ollama
from llama_index.embeddings import HuggingFaceEmbedding

# Load documents
documents = SimpleDirectoryReader('data').load_data()

# Set up local LLM (Ollama)
llm = Ollama(model="llama2")

# Set up local embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

# Create index
index = VectorStoreIndex.from_documents(
    documents, 
    llm=llm,
    embed_model=embed_model
)

# Query engine
query_engine = index.as_query_engine()
response = query_engine.query("Your question here")
print(response)